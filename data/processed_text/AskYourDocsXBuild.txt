Step 1: Set Up Your Environment  
â€¢ 
Create a Python virtual environment:  
bash 
CopyEdit  
python3 -m venv askyourdocs-env  
source askyourdocs-env/bin/activate   # Linux/Mac  
# or  
askyourdocs-env\Scripts\activate      # Windows  
â€¢ 
Install essential packages:  
bash 
CopyEdit  
pip install langchain llama-index sentence-transformers faiss-cpu 
pytesseract opencv-python pdfplumber python-docx streamlit 
networkx plotly redis auto-gen crewai  
â€¢ 
Install Tesseract OCR on your machine:  
o Ubuntu: sudo apt install tesseract-ocr o Windows: Download installer 
from https://github.com/tesseract-ocr/tesseract  
  
Step 2: Create Project Folder Structure  
Create these folders and files:  
arduino CopyEdit 
askyourdocs-x/  
â”œâ”€â”€ agents/  
â”‚   â”œâ”€â”€ context_miner.py  
â”‚   â”œâ”€â”€ contradiction_hunter.py  
â”‚   â”œâ”€â”€ action_planner.py  
â”‚   â””â”€â”€ persona_shifter.py  
â”œâ”€â”€ core/  
â”‚   â”œâ”€â”€ document_parser.py  
â”‚   â”œâ”€â”€ ocr_processor.py  
â”‚   â”œâ”€â”€ chunker.py  
â”‚   â”œâ”€â”€ embedder.py  
â”‚   â”œâ”€â”€ vectorstore.py  
â”‚   â”œâ”€â”€ rag_chain.py  
â”‚   â”œâ”€â”€ memory_store.py  
â”‚   â”œâ”€â”€ temporal_reasoning.py  
â”‚   â””â”€â”€ org_graph_builder.py  
â”œâ”€â”€ frontend/  
â”‚   â”œâ”€â”€ app.py  
â”‚   â””â”€â”€ components.py  
â”œâ”€â”€ utils/  
â”‚   â”œâ”€â”€ config.py  
â”‚   â”œâ”€â”€ logging.py  
â”‚   â””â”€â”€ prompt_templates.py  
â”œâ”€â”€ data/  
â”‚   â”œâ”€â”€ uploads/  
â”‚   â”œâ”€â”€ processed_text/  
â”‚   â”œâ”€â”€ embeddings/  
â”‚   â””â”€â”€ graphs/  
â”œâ”€â”€ tests/  
â”‚   â”œâ”€â”€ test_agents.py  
â”‚   â”œâ”€â”€ test_core.py  
â”‚   â””â”€â”€ test_frontend.py  
â”œâ”€â”€ requirements.txt  
â””â”€â”€ README.md  
  
Step 3: Implement Document Parsing (core/document_parser.py) â€¢ 
Use pdfplumber to extract text from PDFs.  
 
â€¢  Use python-docx for Word docs.  
Example:  
python CopyEdit import 
pdfplumber from docx 
import Document  
 def 
extract_text_from_pdf(path):  
    text = ""     with 
pdfplumber.open(path) as pdf:         
for page in pdf.pages:  
            text += page.extract_text() + "\n"     
return text  
 def 
extract_text_from_docx(path):  
    doc = Document(path)  
    return "\n".join([para.text for para in doc.paragraphs])  
  
Step 4: Implement OCR for Scanned Images (core/ocr_processor.py)  
 
â€¢  Use pytesseract + OpenCV to extract text from scanned images.  
Example:  
python CopyEdit 
import cv2 import 
pytesseract  
 def ocr_from_image(image_path):     
img = cv2.imread(image_path)  
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)     
text = pytesseract.image_to_string(gray)     return 
text  
  
Step 5: Chunk Text into Pieces (core/chunker.py)  
 
â€¢  Use LangChain's text splitter or your own function.  
Example:  
python CopyEdit  
from langchain.text_splitter import RecursiveCharacterTextSplitter  
 def 
chunk_text(text):  
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, 
chunk_overlap=50)  
    return splitter.split_text(text)  
  
Step 6: Create Embeddings (core/embedder.py)  
 
â€¢  Use SentenceTransformers model to convert text chunks to vectors.  
Example:  
python CopyEdit  
from sentence_transformers import SentenceTransformer  
  
model = SentenceTransformer('all-MiniLM-L6-v2')  
 def 
embed_chunks(chunks):  
    embeddings = model.encode(chunks)     
return embeddings  
  
Step 7: Build Vector Store (core/vectorstore.py)  
 
â€¢  Use FAISS or Qdrant for vector similarity search.  
Example with FAISS:  
python CopyEdit 
import faiss 
import numpy as np  
 def build_faiss_index(embeddings):     
dimension = embeddings.shape[1]     
index = faiss.IndexFlatL2(dimension)     
index.add(np.array(embeddings))     
return index  
 def search_index(index, query_embedding, 
k=5):  
    distances, indices = index.search(np.array([query_embedding]), k)     
return indices[0]  
  
Step 8: Retrieval Augmented Generation Pipeline (core/rag_chain.py)  
â€¢  Use LangChain with your vector store + open-source LLM (e.g., Mistral via 
HuggingFaceHub).  
Example:  
python CopyEdit  
from langchain.chains import RetrievalQA from 
langchain.llms import HuggingFaceHub from 
langchain.vectorstores import FAISS  
from langchain.embeddings import HuggingFaceEmbeddings  
 def 
build_qa_chain(faiss_index):  
    retriever = FAISS(faiss_index, HuggingFaceEmbeddings())     llm = 
HuggingFaceHub(repo_id="mistralai/Mistral-7B-Instruct-v0.1", 
model_kwargs={"temperature":0})  
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)     
return qa  
 def ask_question(qa_chain, 
question):     return 
qa_chain.run(question)  
  
Step 9: Implement Multi-Agent System (agents/ folder)  
 
â€¢  Build 4 agents:  
o 
context_miner.py: Extract entities, timelines. o 
contradiction_hunter.py: Detect contradictions. o 
action_planner.py: Generate actionable items.  
o 
persona_shifter.py: Adapt answers per department/persona.  
Use AutoGen or CrewAI for communication between agents.  
  
Step 10: Add Memory and Temporal Reasoning (core/memory_store.py & 
temporal_reasoning.py)  
 
â€¢  Use Redis or local JSON/SQLite for:  
o Storing uploaded docs, interaction history.  
o Comparing versions for change detection.  
  
Step 11: Build Organizational Knowledge Graph (core/org_graph_builder.py)  
 
â€¢  Use NetworkX + Plotly to create and visualize entity relationships.  
  
Step 12: Create Frontend with Streamlit (frontend/app.py + components.py)  
â€¢ 
Drag & drop upload for docs + images.  
â€¢ 
Chat interface to ask questions.  
â€¢ 
Display knowledge graph.  
â€¢ 
Show action plan summaries.  
Run with:  
bash CopyEdit  
streamlit run frontend/app.py  
  
Step 13: Optional - Integrate with Jira/Notion/Slack (utils/config.py) â€¢ 
Use their APIs to push AI-generated tasks or summaries.  
  
Step 14: Testing & Debugging  
â€¢ 
Write unit tests for each module in tests/ folder.  
â€¢ 
Use logging in utils/logging.py to trace issues.  
  
Step 15: Final Packaging & Demo  
â€¢ 
Prepare README with setup and usage instructions.  
â€¢ 
Create architecture diagrams (architecture.png).  
â€¢ 
Record 2-3 min demo video showing system features.  
â€¢ 
Push code to GitHub with clear commit history.  
  
 
 
2nd project  
4. GenAI for Accessibility: "VisionToSpeech" (Blind Assistant Copilot) (ğŸ§  Vision + GPT + Audio 
Streaming) 
  â€œA live camera-powered GPT assistant that narrates the world around you for visually 
impaired users.â€ 
â€¢ 
What it does: Uses a webcam or phone cam to continuously send frames to a Vision-
GPT model (e.g., GPT-4o), which gives audio narration and answers queries like: 
"Where am I?", "What color is this note?", etc. 
â€¢ 
Why unique: Itâ€™s not just image-to-textâ€”it's interactive narration (open-ended 
Q&A on live vision). 
â€¢ 
Tech: GPT-4o, Whisper (voice), OpenCV, PyTorch + real-time streaming.